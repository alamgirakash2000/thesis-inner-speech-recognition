{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9635c56f",
   "metadata": {},
   "source": [
    "# Dataset Introduction\n",
    "\n",
    "This notebook is trying to replicate the [Github Rebo](https://github.com/N-Nieto/Inner_Speech_Dataset) of the `Inner Speech Classification` Dataset. Maily the tutorial notebook.\n",
    "\n",
    "[Notebook Link](https://github.com/N-Nieto/Inner_Speech_Dataset/blob/main/Database_load_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a43b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !git clone https://github.com/N-Nieto/Inner_Speech_Dataset -quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ea930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_extractions import  Extract_data_from_subject\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import  Select_time_window, Transform_for_classificator, Split_trial_in_time\n",
    "\n",
    "np.random.seed(23)\n",
    "\n",
    "mne.set_log_level(verbose='warning') #to avoid info at terminal\n",
    "warnings.filterwarnings(action = \"ignore\", category = DeprecationWarning )\n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77672aee",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318bc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "datatype = \"EEG\"     # Data Type\n",
    "fs = 256             # Sampling rate\n",
    "t_start = 1.5        # Select the useful par of each trial. Time in seconds\n",
    "t_end = 3.5\n",
    "N_S = 6              # Subject number [1 to 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98aba449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: [trials x channels x samples]:  (540, 128, 512)\n",
      "Labels shape:  (540, 4)\n"
     ]
    }
   ],
   "source": [
    "#@title Data extraction and processing\n",
    "# The root dir have to point to the folder that cointains the database\n",
    "root_dir = \"dataset/inner-speech-recognition\"\n",
    "\n",
    "# Load all trials for a sigle subject\n",
    "X, Y = Extract_data_from_subject(root_dir, N_S, datatype)\n",
    "\n",
    "# Cut usefull time. i.e action interval\n",
    "X = Select_time_window(X = X, t_start = t_start, t_end = t_end, fs = fs)\n",
    "\n",
    "\n",
    "print(\"Data shape: [trials x channels x samples]: \",X.shape)\n",
    "print(\"Labels shape: \",Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82dfe1",
   "metadata": {},
   "source": [
    "## Create the different groups for a classifier. A group is created with one condition and one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf3daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data shape:  (216, 128, 512)\n",
      "Final labels shape:  (216,)\n"
     ]
    }
   ],
   "source": [
    "Conditions = [[\"Inner\"],[\"Inner\"],[\"Inner\"],[\"Inner\"]]    # Conditions to compared\n",
    "Classes    = [  [\"Up\"] ,[\"Down\"],[\"Right\"],[\"Left\"]]    # The class for the above condition\n",
    "\n",
    "# Transform data and keep only the trials of interes\n",
    "X_trans , Y_trans =  Transform_for_classificator(X, Y, Classes, Conditions)\n",
    "\n",
    "print(\"Final data shape: \",X_trans.shape)\n",
    "print(\"Final labels shape: \",Y_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead59d75",
   "metadata": {},
   "source": [
    "# Writing a function to merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb6bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the X_train x Y_train  (1799, 128, 512)  x  (1799,)\n",
      "The shape of the X_val x Y_val  (223, 128, 512)  x  (223,)\n",
      "The shape of the X_test x Y_test:  (214, 128, 512)  x  (214,)\n",
      "CPU times: total: 18.2 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The Storage variables with their respected names\n",
    "X_train =[]\n",
    "Y_train =[]\n",
    "X_val = []\n",
    "Y_val =[]\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "### Hyperparameters\n",
    "datatype = \"EEG\"     # Data Type\n",
    "fs = 256             # Sampling rate\n",
    "t_start = 1.5        # Select the useful par of each trial. Time in seconds\n",
    "t_end = 3.5\n",
    "\n",
    "# Setting parameters\n",
    "root_dir = \"dataset/inner-speech-recognition\"             # path to the main directory of the subjects\n",
    "subject_numbers = [1,2,3,4,5,6,7,8,9,10]                  # Number of subjects\n",
    "Conditions = [[\"Inner\"],[\"Inner\"],[\"Inner\"],[\"Inner\"]]    # Paradigm of data that want to classify\n",
    "Classes = [  [\"Up\"] ,[\"Down\"],[\"Right\"],[\"Left\"]]         # The four clases of data\n",
    "\n",
    "\n",
    "# Main for loop to extract the data and append it\n",
    "for s_n in subject_numbers:\n",
    "    X_sub, Y_sub = Extract_data_from_subject(root_dir, s_n, datatype)\n",
    "    X_sub = Select_time_window(X = X_sub, t_start = t_start, t_end = t_end, fs = fs)\n",
    "    X_transformed , Y_transformed =  Transform_for_classificator(X_sub, Y_sub, Classes, Conditions)\n",
    "    \n",
    "    ## Separating for testing set\n",
    "    indices = np.arange(10, len(X_transformed), 10)\n",
    "    X_tt = X_transformed[indices].copy()\n",
    "    Y_tt = Y_transformed[indices].copy()\n",
    "    # Removed after separating for testing set \n",
    "    X_t =  np.delete(X_transformed, indices, axis =0)\n",
    "    Y_t =  np.delete(Y_transformed, indices)\n",
    "    \n",
    "    ## Separating for Validation set\n",
    "    indices = np.arange(9, len(X_t), 9)\n",
    "    X_tt_val = X_t[indices].copy()\n",
    "    Y_tt_val = Y_t[indices].copy()\n",
    "    # Removed after separating for testing set \n",
    "    X_t =  np.delete(X_t, indices, axis =0)\n",
    "    Y_t =  np.delete(Y_t, indices)\n",
    "       \n",
    "    # Appending data to the main sets\n",
    "    X_train.append(X_t)\n",
    "    Y_train.append(Y_t)\n",
    "    X_val.append(X_tt_val)\n",
    "    Y_val.append(Y_tt_val)\n",
    "    X_test.append(X_tt)\n",
    "    Y_test.append(Y_tt)\n",
    "\n",
    "# Vertically Concatenate all the data of one subject after another\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "Y_val = np.concatenate(Y_val, axis=0)\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "\n",
    "# Printing the shape\n",
    "print(\"The shape of the X_train x Y_train \", X_train.shape , \" x \",Y_train.shape)\n",
    "print(\"The shape of the X_val x Y_val \", X_val.shape , \" x \",Y_val.shape)\n",
    "print(\"The shape of the X_test x Y_test: \", X_test.shape, \" x \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c81b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1799, 128, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cbfde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1799,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e88eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3\n",
       "0   0.0   1.0   2.0   3.0\n",
       "1  47.0  55.0  57.0  55.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "df=pd.DataFrame([unique, counts])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f88458",
   "metadata": {},
   "source": [
    "## Processing\n",
    "The processing was developed in Python, using mainly the MNE library.\n",
    "\n",
    "Using the `Inner_speech_processing.py` script, you can easily make your own processing, changing the variables at the top of the script.\n",
    "\n",
    "The `TFR_representation.py` generates the Time Frequency Representations used addressing the same processing followed in the paper.\n",
    "\n",
    "By means of the `Plot_TFR_Topomap.py` the same images presented in the paper can be addressed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
